{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Documentation -- Milestone 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Differentiation is great. It is a necessity in a vast range of applications, such as atomic simulations, economic analysis, and machine learning. \n",
    "\n",
    "In the field of computational science, there are three ways to differentiate: numerical, symbolic and automatic. \n",
    "\n",
    "Numerical differentiation finds the derivative using finite difference approximations $\\Delta f / \\Delta x$. Even with higher-order methods, its error is far greater than machine precision.\n",
    "\n",
    "Symbolic differentiation finds the symbolic expression of the derivative. When functions and programs get complicated, it becomes inefficient and messy. This is called expression swell.\n",
    "\n",
    "Automatic differentiation can find the derivative of expressions to the accuracy of machine precision. It does not have the problem of expression swell because it deals with numbers. That is why we need automatic differentiation!\n",
    "\n",
    "Our `superjacob` package performs automatic differentiation on single- or multi-variable functions using the _forward mode_ as well as the _reverse mode_. The function is stored as an `Expression` object that can output values and derivatives at any given point. The package is named after Karl Jacob Jacobi, the mathematician who invented the Jacobian matrix. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "Differentiation is the process of finding derivative, which is the rate of change of a function's output with regard to its variables. Take $f(x,y) =3*x^2+\\exp(y)$ as an example. Symbolic differentiation gives $\\dfrac{\\partial f}{\\partial x}=6x$ and $\\dfrac{\\partial f}{\\partial y}=\\exp(y)$.\n",
    "\n",
    "Automatic differentiation treats a function as a chain of elementary functions and performs differentiation on each elementry function. \n",
    "Here the elementary functions include: (1) A single arithmetic operation, such as $3*x$ and $x_1+x_2$. (2) A single trigonometric operation, such as $\\sin(x)$. (3) A single exponential or logarithmic opration, such as $\\log(x)$.\n",
    "\n",
    "The chain rule dictates that \n",
    "\n",
    "$$\\frac{df(g(x))}{dx}=\\frac{df(x_1)}{dx_1}*\\frac{dg(x)}{dx}.$$\n",
    "\n",
    "Therefore, a function that is made up of elementary functions can be extended into a computational graph. For $f(x,y) =3*x^2+\\exp(y)$, the graph is shown below. Each $x_i$ is the output of an elementary function.\n",
    "\n",
    "<img src=\"https://i.imgur.com/hBQvv4n.jpg\" alt=\"drawing\" width=\"600\"/>\n",
    "  \n",
    "To calulate the derivative of $f$ at $[x,\\ y]$, we pass the value of the previous $x_i$ and $x_i^\\prime$ into the next elementary function to evaluate the derivative of that elementary function. Below shows the forward mode automatic differentiation table (traceplot). \n",
    "\n",
    "<img src=\"https://i.imgur.com/1AIngxT.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "The derivative is computed using the chain rule. To get $\\dfrac{\\partial f}{\\partial x}$, forward mode starts from $\\dfrac{\\partial x_1}{\\partial x}$, while the reverse mode starts from $\\dfrac{\\partial x_6}{\\partial x_4}$. \n",
    "\n",
    "Let's do the forward mode at a given point x=2, y=3. Start from the begining, plug in numbers for each step, we have\n",
    "\n",
    "$$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial x_6}{\\partial x_4}\\dfrac{\\partial x_4}{\\partial x_3}\\dfrac{\\partial x_3}{\\partial x_1}\\dfrac{\\partial x_1}{\\partial x}=1*3*4*1=12.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to use superjacob\n",
    "\n",
    "Our goal is for the syntax of `superjacob` to be as natural as possible, not requiring the user to learn any new paradigms and thereby minimizing the chances of errors. Therefore, we take inspiration from the kind of notation one might use when writing out mathematical expressions and functions by hand. \n",
    "\n",
    "The core functionality of `superjacob` involves three main kinds of objects: `Variable`, `Expression`, and subclasses of `Operation`. These mean exactly what you might expect from a mathematical context. If a user wants to define an expression, they first define one or more `Variable`s. The they make an `Expression` using basic math operators such as `+ - * / ` or special operators such as `superjacob.log`. The expression can be evaluated and differntiated at any given point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How to install `superjacob`\n",
    "\n",
    "There are two ways to install `superjacob`:\n",
    "\n",
    "1\\. PyPi\n",
    "\n",
    "In the commmand window, type \n",
    "\n",
    "```python\n",
    "pip install superjacob\n",
    "```\n",
    "\n",
    "2\\. GitHub \n",
    "\n",
    "First, clone our GitHub repository (https://github.com/ADMonsters/cs207-FinalProject.git).\n",
    "\n",
    "(Optional) Activate a virtual enviromment using `conda` or `virtualenv`.\n",
    "\n",
    "In the command window, cd to our repositiory folder. Then type \n",
    "\n",
    "```python\n",
    "pip install -r requirements.txt\n",
    "\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Demo for single variable function\n",
    "```python\n",
    "import superjacob as sj\n",
    "\n",
    "# Define a variable\n",
    "x = sj.Var('x')\n",
    "\n",
    "# A variable behaves like the identity operation\n",
    "x(-24)\n",
    ">>> -24\n",
    "\n",
    "# It has a derivative of 1\n",
    "x.deriv(-24)\n",
    ">>> 1\n",
    "\n",
    "# You can easily create an expression object\n",
    "f = (x * 0.2 + sj.exp(x) / 3) / x\n",
    "\n",
    "# We can evaluate it just like a function\n",
    "f(2)\n",
    ">>> 1.431509349821775\n",
    "\n",
    "# As well as take the derivative using forward mode\n",
    "f.deriv(2)\n",
    ">>> 0.6157546749108876\n",
    "\n",
    "# To define more complex functions, we can combine previously-made expressions\n",
    "g = f + sj.log(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Demo for multi-variable function and vector function\n",
    "\n",
    "```python\n",
    "import superjacob as sj\n",
    "\n",
    "# Define multiple variables\n",
    "x = sj.Var('x')\n",
    "y = sj.Var('y')\n",
    "\n",
    "# Use make_expression to make a expression\n",
    "f = sj.make_expression(x*y + x/y, vars = [x, y])\n",
    "\n",
    "# Values and derivatives\n",
    "f(2, 4)\n",
    ">>> 8.5\n",
    "\n",
    "f.deriv(2, 4)\n",
    ">>> array([4.25 , 1.875])\n",
    "\n",
    "# You can specify the order of variables. Here we swap the order of x and y\n",
    "g = sj.make_expression(x*y + x/y, vars = [y, x])\n",
    "f(2, 4) == g(4, 2)\n",
    ">>> True\n",
    "\n",
    "# To make a vector function, separate scalar functions by a comma\n",
    "fv = sj.make_expression(x*y, x/y, sj.log(x,y), vars = [x, y])\n",
    "\n",
    "# Vector function derivative is a Jacobian\n",
    "fv(2,4)\n",
    ">>> [8, 0.5, 0.5]\n",
    "\n",
    "fv.deriv(2,4)\n",
    ">>> array([[ 4.      ,    2.   ],\n",
    "        [ 0.25     ,  -0.125  ],\n",
    "        [ 0.36067376, -0.09016844]])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Demo for `__str__` and `__eq__` methods\n",
    "\n",
    "```python\n",
    "import superjacob as sj\n",
    "\n",
    "x = sj.Var('x')\n",
    "\n",
    "f = x**3 + sj.log(x,2) - sj.cos(x)\n",
    "\n",
    "# You can print the expression\n",
    "print(f)\n",
    ">>> x^3 + log_2(x) - cos(x)\n",
    "\n",
    "# Two expressions are equal if their strings are equal. But they are not equal if the commutative law is applied\n",
    "g = x**3 + sj.log(x,2) - sj.cos(x)\n",
    "gc = x**3 - sj.cos(x) + sj.log(x,2)\n",
    "\n",
    "f == g\n",
    ">>> True\n",
    "\n",
    "f == gc\n",
    ">>> False\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. List of all math functions \n",
    "\n",
    "```python\n",
    "\n",
    "# Addition, subtraction, multiplication, division, and power are binary operations\n",
    "f = x + 10\n",
    "f = x - 10\n",
    "f = x * 10\n",
    "f = x / 10\n",
    "f = x ** 10\n",
    "\n",
    "# Negation is a unary operation\n",
    "f = - x\n",
    "\n",
    "# Square root is a unary operation\n",
    "f = sj.sqrt(x)\n",
    "\n",
    "# The base in log can be defined. Default base = e\n",
    "f = sj.log(x)\n",
    "f = sj.log(x, 10)\n",
    "\n",
    "# Trigonometric functions are unary operations\n",
    "f = sj.sin(x)\n",
    "f = sj.cos(x)\n",
    "f = sj.tan(x)\n",
    "f = sj.sec(x)\n",
    "f = sj.csc(x)\n",
    "f = sj.cot(x)\n",
    "f = sj.arcsin(x)\n",
    "f = sj.arccos(x)\n",
    "f = sj.arctan(x)\n",
    "\n",
    "# Hyperbolic functions are unary operations\n",
    "f = sj.sinh(x)\n",
    "f = sj.cosh(x)\n",
    "f = sj.tanh(x)\n",
    "\n",
    "# Logistic function. Default is k=1, x0=0, L=1\n",
    "f = sj.logistic(x)\n",
    "f = sj.logistic(x, k=2, x0=2, L=2)\n",
    "\n",
    "# Currently superjacob does not support complex numbers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Demo for reverse mode\n",
    "\n",
    "```python\n",
    "\n",
    "# Choose forward and reverse in the deriv method. Default is \n",
    "# forward mode for single variable functions and reverse for multivariable functions\n",
    "f= x**3\n",
    "f.deriv(2, mode = 'forward')\n",
    "f.deriv(2, mode = 'reverse')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Software Organization\n",
    "### 1. Directory structure\n",
    "```\n",
    "cs207-FinalProject/\n",
    "|\n",
    "|-- superjacob/\n",
    "|   |-- __init__.py\n",
    "|   |-- superjacob.py\n",
    "|   |-- expression.py\n",
    "|   |-- operations.py\n",
    "|   |-- reverse.py\n",
    "|\n",
    "|-- tests/\n",
    "|   |-- test_operations.py\n",
    "|   |-- test_expression.py\n",
    "|   |-- test_expression_multivar.py\n",
    "|\n",
    "|-- docs/\n",
    "|   |-- milestone1.md\n",
    "|   |-- milestone2.ipynb\n",
    "|   |-- documentation.ipynb\n",
    "|\n",
    "|-- README.md\n",
    "|-- requirements.txt\n",
    "|-- setup.py\n",
    "|-- LICENSE\n",
    "```\n",
    "\n",
    "`superjacob` subdirectory hosts our code. It is set up as a package through `__init__.py`.\n",
    "\n",
    "`tests/` subdirectory hosts tests to the code.\n",
    "\n",
    "`docs/` subdirectory hosts our documents. The documents provides an introduction to automatic differntiation, as well as a guide to using our package.\n",
    "\n",
    "### 2. Basic modules and their functionality\n",
    "\n",
    "Our modules are `superjacob.py`, `expression.py`, and `operations.py`. \n",
    "\n",
    "1. `superjacob.py` imports everthing from `expression.py` and `operations.py`. This contains the user's primary interface through the `make_expression` function, which abstracts away the graph structure of autodiff and easily allow the user to create an expression. It also contains a set of basic math operations the user can utilize in their expressions, like $sin(x)$ or $exp(x)$. \n",
    "\n",
    "2. `expression.py` contains the code for our `Variable` and `Expression` classes. These objects build the computational graph in a tree-like structure. Users can call the `eval()` and `deriv()` methods of an expression to get values and derivatives.\n",
    "\n",
    "3. `operations.py` contains elementary function classes. Each elementary function have methods to build new expressions, to evaluate at given points, and to compute the derivative at given points.\n",
    "\n",
    "4. `reverse.py` contains classes to perform the reverse mode differentiation.\n",
    "\n",
    "### 3. Where do tests live? How are they run? How are they integrated?\n",
    "\n",
    "As shown in the directory structure, our test suite lives inside the `tests/` subdirectory. \n",
    "\n",
    "The module names indicate what they are testing. In each module, there are many unit tests to ensure the differentiator run correctly and handle edge cases, including type checking, appropriately for a variety of complex functions. They also ensure that basic math operators are properly overloaded when users create expressions and evaluate them at certain points.\n",
    "\n",
    "- `test_operations.py` more closely tests many of the math operations abstracted away under the hood to ensure quality is maintained beneath all the layers.\n",
    "\n",
    "- `test_expression.py` and `test_expression_multivar.py` more closely ensures that `Expression` objects are instantiated properly and return correct evaluations and deriatives. \n",
    "\n",
    "`pytest` runs the test suite. TravisCI performed integration testing as we built the package to ensure proper functionality among all the moving pieces in our code, and it helped flag defects as they arose and maintained quality control among the various components in the software. In addition, CodeCov helped us analyze ways to improve our test suite to maintain high coverage of our code.\n",
    "\n",
    "### 4. How to install our package\n",
    "\n",
    "How to install is in the previous section. To develop this package, a user should activate a virtual environment. Then in the repository folder, type\n",
    "```python\n",
    "pip install -r requirements.txt\n",
    "pip install .\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementation\n",
    "### 1. Core data structures\n",
    "\n",
    "The function to be differentiated (henceforth referred to as ƒ) will be parsed and converted into a directed graph, containing node-like objects for each step in the traceplot (i.e. each node represents an elementary operation in ƒ). The edges of the graph represent steps from one part of the traceplot to the next. This is similar to the tree structure that we learned in class, but it is built from the leaf nodes to the root.\n",
    "\n",
    "Each node is an `Expression` object that contains\n",
    "- The type of elementary operation being performed. It could be add, mul, pow, log, etc. \n",
    "- References to the mathematical objects ('parents') that go into this operation. A binary operation has two parents, while a unary operation has only one parent. The parent could be an `Expression` object, a `Variable` object, or just a number.\n",
    "- Notice that a node does not reference the next operation to be performed. \n",
    "\n",
    "Say we have the following situation:\n",
    "\n",
    "![](https://i.imgur.com/p2gMe9B.png)\n",
    "\n",
    "\n",
    "In this case, C knows about A and B, but not about F. This may seem counterintuitive, since in the forward mode of autodiff, we need to go from A to C to F. However, we want to allow for situations like the following:\n",
    "\n",
    "![](https://i.imgur.com/eWljQhb.png)\n",
    "\n",
    "\n",
    "Here, the function `f3` is composed of two inputs (`f1` and `f2`) and combines them in an operation in node G. Rather than copying functions `f1` and `f2` into brand new graphs, we believe that it would be more memory- and time-efficient to simply refer to the same graph objects that `f1` and `f2` refer to. However, this creates a potential issue if we were to implement the graphs as bidirectional, rather than unidirectional: if we add a connection from C to G, then if the user tries to run the forward mode of autodiff on `f1`, the algorithm will continue past node C onto node G. However, node G is not part of function `f1`!\n",
    "\n",
    "This design choice has a tradeoff, namely that each time forward mode auto differentiation is performed, Python must step from the end of the function all the way to the beginning leaf nodes. This is done by recursively calling the `eval()` and `deriv()` methods.\n",
    "\n",
    "### 2. Core classes\n",
    "\n",
    "<img src=\"https://i.imgur.com/ST3mu2D.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "\n",
    "`Var` class is the variable in the eventual function. It overloads math operators such as `__add__` and `__pow__`.\n",
    "\n",
    "\n",
    "`Expression` class inherits from `Var`. It stores the user-defined functions. As shown in the demo, `f` and `g` are expressions.\n",
    "\n",
    "\n",
    "`BaseOperation` and its subclasses. Each operation has class methods to return new expression objects, to return the numerical value, and to return the drivative.\n",
    "\n",
    "\n",
    "### 3. Important attributes\n",
    "\n",
    "`Var` class\n",
    "\n",
    "- `eval()` method. Var object do not store values. It returns whatever value that is being passe into it.\n",
    "- `deriv()` method. If we are differentiating with respect to this particular variable, it returns 1. If we are differentiating with respect to another variable, it returns 0. This is done by passing a boolean as the input.\n",
    "- Math operators such as `__add__()`. It calls `superjacob.add`, which returns an Expression object.\n",
    "\n",
    "\n",
    "`Expression` class\n",
    "\n",
    "- All methods defined in `Variable`\n",
    "- `self.parents`. The parents can be an Expression object, a Var object, or a number.\n",
    "- `self.operation` stores the operation that is being performed.\n",
    "- `self._vars` stores the list of variables but do not store their values. \n",
    "- `eval()` and `deriv()` use helper methods to recursively find the value and derivative at any given point. Currently `deriv` only works with the forward mode.\n",
    "- `_parse_args()` and `_get_input_args()` are helper methods that take in numerical input and smartly passes the values to the parent expressions. They can handle the situation where two parent expressions use different variables, e.g. `f(x) * g(y)`.\n",
    "\n",
    "\n",
    "Operation classes such as `Add` and `Log`\n",
    "\n",
    "- `expr()` takes in parent nodes and returns a new Expression object.\n",
    "- `eval()` takes in numbers to evaluate this operation at any given point.\n",
    "- `deriv()` takes in numbers to find the derivative at any given point. For example, `Mul.deriv()` returns `num1 * deriv2 + num2 * deriv1`.\n",
    "\n",
    "`make_expression` function \n",
    "\n",
    "- It returns an expression with the varlist in the specified order. In this way, a user can control the order of the variable list.\n",
    "\n",
    "```python\n",
    "x = sj.Var('x')\n",
    "y = sj.Var('y')\n",
    "f1 = sj.make_expression(sd.log(x)/ y, vars = [x, y])\n",
    "f2 = sj.make_expression(sd.log(x)/ y, vars = [y, x])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 4. External dependencies\n",
    "\n",
    "* `numpy`\n",
    "* `pytest` (only for the test suite, not for the actual functionality)\n",
    "\n",
    "### 5. Elementary functions\n",
    "\n",
    "The elementary functions are coded in module `operations.py`. As described above, each operation has its own class. All the operations are listed in the how to use section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Feature - Reverse Mode\n",
    "\n",
    "In many cases, the function to be differentiated is a multivariate function with many inputs and few outputs. This is a common situation in machine learning, where loss functions take as parameters all the weights (in the case of a neural net) or coefficients (in the case of a regression) and output a scalar loss value. The major limitation of forward mode is that it requires a pass through the computational graph for each input, quickly rendering functions of this kind too costly to differentiate.\n",
    "\n",
    "**Reverse mode** of automatic differentiation avoids this flaw by breaking up the differentiation process into two components: a *forward pass* and a *reverse pass*. As will be explained subsequently, the result is an algorithm that is able to compute the derivative of the function with respect to each input variable in a single loop, resulting in significant computational gains.\n",
    "\n",
    "It is for this reason that **reverse mode** is often the more commonly used flavor of automatic differentiation and is usually referred to by the moniker **backpropagation**.\n",
    "\n",
    "### How reverse mode works\n",
    "\n",
    "As mentioned, reverse mode proceeds in two steps:\n",
    "\n",
    "#### The forward pass\n",
    "\n",
    "The algorithm passes through the computational graph much like forward mode, beginning at the base variables and proceeding through the network until the final output function(s) is (are) reached. Along the way, the value of the function at each node is computed (like in the forward pass). However, the computation of the derivatives is different in that the chain rule is not used. Instead, the derivative of the node is taken only with respect to each parent (i.e. not with respect to each variable). For instance,\n",
    "$$\n",
    "    x_5 = x_3 \\cdot x_4\n",
    "$$\n",
    "Then when we compute the derivative of $x_5$ in the forward pass, we get:\n",
    "$$\n",
    "    \\dot x_5 = \\left(\\frac{d}{dx_3}(x_3 \\cdot x_4), \\frac{d}{dx_4}(x_3\\cdot x_4)\\right) = (x_4, x_3)\n",
    "$$\n",
    "\n",
    "We then evaluate these derivatives at the user-supplied value and continue on through the computational trace.\n",
    "\n",
    "#### The reverse pass\n",
    "\n",
    "In the reverse pass, we take all the computed values and derivatives and we use them to compute the derivative of the overall function $f$ with respect to each step in the trace, going from the last node to the variables at the beginning. Say we have a simple function:\n",
    "$$\n",
    "    f(x, y, z) = x + y \\cdot z\n",
    "$$\n",
    "This would map to a simple computational graph:\n",
    "\n",
    "```\n",
    "x --> x1 --> x5 (+) --> f\n",
    "             ^\n",
    "             |\n",
    "y --> x2 --> x4 (*)\n",
    "             ^\n",
    "z --> x3 ----|\n",
    "```\n",
    "\n",
    "We can simply compute the derivative of the function with respect to each node in the computational trace stepping backwards from $x_4$ to $x_1$:\n",
    "\n",
    "$$\n",
    "    \\bar{x_5} &= \\frac{df}{dx_5} = 1 \\\\\n",
    "    \\bar{x_4} &= \\frac{df}{dx_4} = \\bar{x_5}\\frac{dx_5}{dx_4} \\\\\n",
    "    \\bar{x_3} &= \\frac{df}{dx_3} = \\bar{x_4}\\frac{dx_4}{dx_3} = \\frac{df}{dz} \\\\\n",
    "    \\bar{x_2} &= \\frac{df}{dx_2} = \\bar{x_4}\\frac{dx_4}{dx_2} = \\frac{df}{dy} \\\\\n",
    "    \\bar{x_1} &= \\frac{df}{dx_1} = \\bar{x_5}\\frac{dx_5}{dx_1} = \\frac{df}{dx} \\\\\n",
    "$$\n",
    "\n",
    "When we need to compute, say, $\\frac{dx_4}{dx_3}$, we simply sub in the value we computed in the forward pass!\n",
    "\n",
    "The only complication arises when we have a node in the computational graph with more than one child. For instance:\n",
    "$$\n",
    "    f(x, y) = x^2 + x / y\n",
    "$$\n",
    "Which corresponds to the following graph:\n",
    "\n",
    "```\n",
    "x --> x1 --> x3 (pow)-> x5 (add) --> f\n",
    "        \\               ^\n",
    "         \\              |\n",
    "y --> x2 --> x4 (div) --+\n",
    "```\n",
    "\n",
    "In order to get the derivative of $x_1$, we need to use the chain rule to *add up* the derivative from $x_4$ and $x_3$):\n",
    "$$\n",
    "    \\bar{x_1} = \\frac{df}{dx_1} = \\bar{x_4}\\frac{dx_4}{dx_1} + \\bar{x_3}\\frac{dx_3}{dx_1}\n",
    "$$\n",
    "\n",
    "Once again, the computation of the full derivative of each node in the reverse pass means that we only need to do _one_ forward and _one_ reverse pass per output function, _regardless_ of the number of input functions.\n",
    "\n",
    "### Reverse mode implementation\n",
    "\n",
    "Reverse mode is implemented in `reverse.py`. Two additional classes are created:\n",
    "\n",
    "1) `ReverseDiff`\n",
    "\n",
    "The core responsibility of `ReverseDiff` is to wrap an `Expression` object, beginning the forward and reverse passes and collecting the resulting derivatives into a gradient vector or Jacobian matrix.\n",
    "\n",
    "**Methods and important attributes**\n",
    "\n",
    "`expr`\n",
    "\n",
    "The `Expression` object whose derivative is being taken.\n",
    "\n",
    "`vars`\n",
    "\n",
    "The correct ordering of the `Var` objects for that `Expression`\n",
    "\n",
    "`trace`\n",
    "\n",
    "The nodes in the computational trace. Initially, this is an empty list, but is later populated during a call to `forward`.\n",
    "\n",
    "`forward(self, expr, *args, child=None)`\n",
    "\n",
    "Conducts the foward pass. For each `Expression` or `Var` in the overall function to be differentiated, it instantiates a `TraceNode` (more on this below). By the end of the `forward` call, the `trace` attribute of `ReverseDiff` is populated with the full computational trace (each node is represented by a `TraceNode` object).\n",
    "\n",
    "`reverse(self, var=None)`\n",
    "\n",
    "Conducts the reverse pass. For each `TraceNode` in the `trace` attribute, the derivative of the overall function is computed with respect to that node. If that `TraceNode` represents one of the base variables (i.e. it contains a `Var` object then that derivative is saved to be included in the gradient or Jacobian.\n",
    "\n",
    "2) `TraceNode`\n",
    "\n",
    "The main role of this class is to store the current values and derivatives during the forward pass, as well as the children of that node (since `Expression` objects only contain references to their parents; not their children). This class is also responsible for computing the derivative (`bar`) of the `TraceNode` with respect to the overall function.\n",
    "\n",
    "**Methods and important attributes**\n",
    "\n",
    "`expr`\n",
    "\n",
    "The `Expression` that is represented by this `TraceNode`\n",
    "\n",
    "`currval`\n",
    "\n",
    "The current value of this `TraceNode`. Computed during the forward pass.\n",
    "\n",
    "`derivs`\n",
    "\n",
    "The derivatives of this `TraceNode` with respect to its two parents (a list of length 1 or 2, depending on if this is a unary or binary expression).\n",
    "\n",
    "`children`\n",
    "\n",
    "The `TraceNode`s that represent the children of this `TraceNode`.\n",
    "\n",
    "`add_child(self, child)`\n",
    "\n",
    "Adds a reference to another `TraceNode` that represents the child of this `TraceNode`'s `Expression`. This is what allows the reverse pass to go from children to parents.\n",
    "\n",
    "`bar` (property)\n",
    "\n",
    "Compute the derivative of the overall function with respect to this `TraceNode`. Or, if the derivative has already been computed (and stored), return the derivative of the overall function with respect to this `TraceNode`.\n",
    "\n",
    "`deriv_parent(self, parent_expr)`\n",
    "\n",
    "Compute the derivative of this `TraceNode` with respect to the `Expression` object `parent_expr`. This is called during the reverse pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future plan\n",
    "\n",
    "1\\. Nth Derivatives\n",
    "\n",
    "Give user option to specify arbitrary number of times to differentiate function. This can be done by recursively calling our operators to find the nth derivatives.\n",
    "\n",
    "In the same way, the Jacobian matrix can be extended to the Hessian matrix. Hessian matrix represents 2nd partial derivatives of a vector function.\n",
    "\n",
    "    \n",
    "2\\. Vector Calculus\n",
    "\n",
    "Support divergence and curl operators on vector fields.\n",
    "\n",
    "\n",
    "3.\\ Polar and Spherical Coordinates\n",
    "\n",
    "User can input function in polar or spherical form. Differential operators can return values in the corresponding coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
